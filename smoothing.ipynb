{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205acc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Fashion MNIST Multi-Layer Neural Network from Scratch\n",
    "# ----------------------------------------------------\n",
    "# This script:\n",
    "# 1. Loads the Fashion MNIST dataset from CSV\n",
    "# 2. Splits into train/test sets per class\n",
    "# 3. Preprocesses and normalizes data\n",
    "# 4. Defines ReLU, Sigmoid, Softmax activations\n",
    "# 5. Builds and trains a neural network with L2 regularization and label smoothing\n",
    "# 6. Tracks and prints accuracy and loss across epochs\n",
    "# ----------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Load the CSV containing Fashion MNIST data\n",
    "# ----------------------------------------------------\n",
    "df = pd.read_csv(\"/kaggle/input/fashion-mnist-train-csv/fashion-mnist_train.csv\")\n",
    "\n",
    "# Group by class label to split manually\n",
    "grouped = df.groupby(\"label\")\n",
    "\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "# For each class, split 80% train / 20% test\n",
    "for label, group in grouped:\n",
    "    train_split, test_split = train_test_split(\n",
    "        group,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "        stratify=None\n",
    "    )\n",
    "    train_list.append(train_split)\n",
    "    test_list.append(test_split)\n",
    "\n",
    "# Concatenate all splits and shuffle\n",
    "train_df = pd.concat(train_list).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = pd.concat(test_list).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Visualize one example per class\n",
    "# ----------------------------------------------------\n",
    "examples = train_df.groupby(\"label\").first().reset_index()\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(10):\n",
    "    ax = plt.subplot(2, 5, i + 1)\n",
    "    img = examples.loc[i].drop(\"label\").values.astype(np.uint8).reshape(28, 28)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(f\"Label: {examples.loc[i, 'label']}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Prepare Train/Test Feature Matrices and Labels\n",
    "# ----------------------------------------------------\n",
    "X = train_df.drop(\"label\", axis=1).values.astype(np.float32)\n",
    "y = train_df[\"label\"].values\n",
    "num_classes = np.max(y) + 1\n",
    "y = np.eye(num_classes)[y]  # One-hot encoding\n",
    "\n",
    "X_test = test_df.drop(\"label\", axis=1).values.astype(np.float32)\n",
    "y_test = test_df[\"label\"].values\n",
    "y_test = np.eye(num_classes)[y_test]\n",
    "\n",
    "# Normalize data\n",
    "np.random.seed(0)\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X = (X - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "print(\"Shapes:\", X.shape, y.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Activation Functions\n",
    "# ----------------------------------------------------\n",
    "def relu(x, grad):\n",
    "    if grad:\n",
    "        return (x > 0).astype(float)\n",
    "    else:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x, grad):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    if grad:\n",
    "        return s * (1 - s)\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "def softmax(z, grad):\n",
    "    exps = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Network Architecture and Initialization\n",
    "# ----------------------------------------------------\n",
    "arch = [128, 128, 10]\n",
    "activations = [relu, relu, softmax]\n",
    "W = []\n",
    "B = []\n",
    "\n",
    "alpha = 0.002\n",
    "batch = 32\n",
    "lmbda = 0.1\n",
    "best_test = 0\n",
    "\n",
    "def label_smoothing(y_true, epsilon=0.1):\n",
    "    K = y_true.shape[1]\n",
    "    return (1 - epsilon) * y_true + epsilon / K\n",
    "\n",
    "# He initialization\n",
    "for i in range(len(arch)):\n",
    "    if i == 0:\n",
    "        w = np.random.randn(X.shape[1], arch[i]) * np.sqrt(2. / X.shape[1])\n",
    "    else:\n",
    "        w = np.random.randn(arch[i - 1], arch[i]) * np.sqrt(2. / arch[i - 1])\n",
    "    b = np.zeros((1, arch[i]))\n",
    "    W.append(w)\n",
    "    B.append(b)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Training Loop\n",
    "# ----------------------------------------------------\n",
    "whole_accuracy = []\n",
    "whole_accuracy_test = []\n",
    "whole_cost = []\n",
    "whole_cost_test = []\n",
    "\n",
    "for e in range(600):\n",
    "    all_accuracy = []\n",
    "    all_cost = []\n",
    "\n",
    "    # Training batches\n",
    "    for i in range(int(np.ceil(len(X) / batch))):\n",
    "        X_batch = X[batch * i : batch * (i + 1)]\n",
    "        y_batch = y[batch * i : batch * (i + 1)]\n",
    "        m_batch = X_batch.shape[0]\n",
    "        y_smooth = label_smoothing(y_batch, epsilon=0.1)\n",
    "\n",
    "        # Forward pass\n",
    "        A = X_batch\n",
    "        all_A = []\n",
    "        all_Z = []\n",
    "        for j in range(len(W)):\n",
    "            Z = A @ W[j] + B[j]\n",
    "            A = activations[j](Z, grad=False)\n",
    "            all_Z.append(Z)\n",
    "            all_A.append(A)\n",
    "\n",
    "        # Cost\n",
    "        cost = (-1 / m_batch) * np.sum(y_smooth * np.log(A + 1e-8))\n",
    "        cost += (lmbda / (2 * m_batch)) * sum([np.sum(w ** 2) for w in W])\n",
    "        all_cost.append(cost)\n",
    "\n",
    "        # Accuracy\n",
    "        y_pred = np.argmax(A, axis=1)\n",
    "        y_true = np.argmax(y_smooth, axis=1)\n",
    "        accuracy = np.mean(y_pred == y_true) * 100\n",
    "        all_accuracy.append(accuracy)\n",
    "\n",
    "        # Backpropagation\n",
    "        for j in reversed(range(len(W))):\n",
    "            if j == len(W) - 1:\n",
    "                dz = all_A[j] - y_smooth\n",
    "            else:\n",
    "                dz = (dz @ W[j + 1].T) * activations[j](all_Z[j], grad=True)\n",
    "            dw = X_batch.T @ dz if j == 0 else all_A[j - 1].T @ dz\n",
    "            W[j] -= (alpha / m_batch) * (dw + lmbda * W[j])\n",
    "            B[j] -= (alpha / m_batch) * np.sum(dz, axis=0, keepdims=True)\n",
    "\n",
    "    # Evaluation on test set\n",
    "    all_accuracy_test = []\n",
    "    all_cost_test = []\n",
    "    for k in range(int(np.ceil(len(X_test) / batch))):\n",
    "        X_test_batch = X_test[batch * k : batch * (k + 1)]\n",
    "        y_test_batch = y_test[batch * k : batch * (k + 1)]\n",
    "        m_test = X_test_batch.shape[0]\n",
    "\n",
    "        A = X_test_batch\n",
    "        for j in range(len(W)):\n",
    "            Z = A @ W[j] + B[j]\n",
    "            A = activations[j](Z, grad=False)\n",
    "\n",
    "        cost_test = (-1 / m_test) * np.sum(y_test_batch * np.log(A + 1e-8))\n",
    "        cost_test += (lmbda / (2 * m_test)) * sum([np.sum(w ** 2) for w in W])\n",
    "\n",
    "        y_pred_test = np.argmax(A, axis=1)\n",
    "        y_true_test = np.argmax(y_test_batch, axis=1)\n",
    "        accuracy_test = np.mean(y_pred_test == y_true_test) * 100\n",
    "\n",
    "        all_accuracy_test.append(accuracy_test)\n",
    "        all_cost_test.append(cost_test)\n",
    "\n",
    "    # Record stats\n",
    "    mean_acc = np.mean(all_accuracy)\n",
    "    mean_acc_test = np.mean(all_accuracy_test)\n",
    "    mean_cost = np.mean(all_cost)\n",
    "    mean_cost_test = np.mean(all_cost_test)\n",
    "\n",
    "    whole_accuracy.append(mean_acc)\n",
    "    whole_accuracy_test.append(mean_acc_test)\n",
    "    whole_cost.append(mean_cost)\n",
    "    whole_cost_test.append(mean_cost_test)\n",
    "\n",
    "    # Save best model\n",
    "    if best_test < mean_acc_test:\n",
    "        best_test = mean_acc_test\n",
    "        for k in range(len(W)):\n",
    "            np.save(f\"/kaggle/working/W{k}.npy\", W[k])\n",
    "            np.save(f\"/kaggle/working/B{k}.npy\", B[k])\n",
    "\n",
    "    # Print progress\n",
    "    print(\n",
    "        f\"Epoch {e} | Best Test Acc: {best_test:.2f}% \"\n",
    "        f\"| Train Acc: {mean_acc:.2f}% | Test Acc: {mean_acc_test:.2f}% \"\n",
    "        f\"| Train Cost: {mean_cost:.4f} | Test Cost: {mean_cost_test:.4f}\"\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
